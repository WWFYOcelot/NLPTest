Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=250
Accuracy: 84.37%
              precision    recall  f1-score   support

     Class 0       0.84      0.90      0.87       860
     Class 1       0.85      0.78      0.81       663

    accuracy                           0.84      1523
   macro avg       0.85      0.84      0.84      1523
weighted avg       0.84      0.84      0.84      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=250
Accuracy: 83.52%
              precision    recall  f1-score   support

     Class 0       0.83      0.90      0.86       888
     Class 1       0.84      0.75      0.79       635

    accuracy                           0.84      1523
   macro avg       0.84      0.82      0.83      1523
weighted avg       0.84      0.84      0.83      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=100
Accuracy: 83.45%
              precision    recall  f1-score   support

     Class 0       0.84      0.88      0.86       866
     Class 1       0.83      0.77      0.80       657

    accuracy                           0.83      1523
   macro avg       0.83      0.83      0.83      1523
weighted avg       0.83      0.83      0.83      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=50, Batch Size=16
Accuracy: 85.69%
              precision    recall  f1-score   support

     Class 0       0.86      0.89      0.88       869
     Class 1       0.85      0.81      0.83       654

    accuracy                           0.86      1523
   macro avg       0.86      0.85      0.85      1523
weighted avg       0.86      0.86      0.86      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=25, Batch Size=32
Accuracy: 83.32%
              precision    recall  f1-score   support

     Class 0       0.83      0.89      0.86       873
     Class 1       0.84      0.75      0.79       650

    accuracy                           0.83      1523
   macro avg       0.83      0.82      0.83      1523
weighted avg       0.83      0.83      0.83      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=50, Batch Size=32
Accuracy: 83.91%
              precision    recall  f1-score   support

     Class 0       0.85      0.88      0.86       874
     Class 1       0.83      0.79      0.81       649

    accuracy                           0.84      1523
   macro avg       0.84      0.83      0.83      1523
weighted avg       0.84      0.84      0.84      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=50, Batch Size=16
Accuracy: 84.18%
              precision    recall  f1-score   support

     Class 0       0.86      0.88      0.87       885
     Class 1       0.82      0.79      0.81       638

    accuracy                           0.84      1523
   macro avg       0.84      0.84      0.84      1523
weighted avg       0.84      0.84      0.84      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=50, Batch Size=16
Accuracy: 83.59%
              precision    recall  f1-score   support

     Class 0       0.85      0.87      0.86       869
     Class 1       0.82      0.79      0.80       654

    accuracy                           0.84      1523
   macro avg       0.83      0.83      0.83      1523
weighted avg       0.84      0.84      0.84      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=10, Batch Size=128
Accuracy: 83.32%
              precision    recall  f1-score   support

     Class 0       0.84      0.88      0.86       891
     Class 1       0.82      0.77      0.79       632

    accuracy                           0.83      1523
   macro avg       0.83      0.82      0.83      1523
weighted avg       0.83      0.83      0.83      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=10, Weight Decay=0.01, Warmup Steps=250, Batch Size=16
Accuracy: 81.22%
              precision    recall  f1-score   support

     Class 0       0.84      0.84      0.84       884
     Class 1       0.78      0.77      0.78       639

    accuracy                           0.81      1523
   macro avg       0.81      0.81      0.81      1523
weighted avg       0.81      0.81      0.81      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=5, Weight Decay=0.01, Warmup Steps=250, Batch Size=16
Accuracy: 81.68%
              precision    recall  f1-score   support

     Class 0       0.82      0.87      0.84       850
     Class 1       0.82      0.75      0.78       673

    accuracy                           0.82      1523
   macro avg       0.82      0.81      0.81      1523
weighted avg       0.82      0.82      0.82      1523

=================================================================
Model: BERTweet with LR=1e-05, Epochs=3, Weight Decay=0.01, Warmup Steps=10, Batch Size=128
Accuracy: 82.93%
              precision    recall  f1-score   support

     Class 0       0.83      0.87      0.85       859
     Class 1       0.82      0.77      0.80       664

    accuracy                           0.83      1523
   macro avg       0.83      0.82      0.83      1523
weighted avg       0.83      0.83      0.83      1523

=================================================================
